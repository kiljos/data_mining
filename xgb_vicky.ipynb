{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ad697a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Version 1 XGBoost + One-Hot Encoding\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from Preprocessing.split import split_data\n",
    "from Preprocessing.preprocessing_pipeline_initial import preprocessing_pipeline\n",
    "\n",
    "def train_final_xgboost_regression():\n",
    "    df = preprocessing_pipeline()\n",
    "\n",
    "    # Train-Test-Split\n",
    "    X_train, X_test, y_train, y_test, X, y, _, _ = split_data(df) \n",
    "\n",
    "    # Stichprobe ziehen\n",
    "    print(\"\\nVerwende nur Stichprobe (10.000 Train, 2.000 Test)\")\n",
    "    X_train = X_train.sample(n=10000, random_state=42)\n",
    "    y_train = y_train.loc[X_train.index]\n",
    "\n",
    "    X_test = X_test.sample(n=2000, random_state=42)\n",
    "    y_test = y_test.loc[X_test.index]\n",
    "\n",
    "    # Features einmal anzeigen (für Fehlerfindung, falls nötig)\n",
    "    numeric_features = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "    categorical_features = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "    print(\"Erkannte numerische Features:\", numeric_features)\n",
    "    print(\"Erkannte kategorische Features:\", categorical_features)\n",
    "\n",
    "    # Skalierung, diesmal One-Hot-Encoding\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "    ])\n",
    "\n",
    "    preprocessor = ColumnTransformer(transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "    # Anwendung Preprocessing\n",
    "    print(\"Preprocessing wird angewendet...\")\n",
    "    X_train_transformed = preprocessor.fit_transform(X_train)\n",
    "    X_test_transformed = preprocessor.transform(X_test)\n",
    "\n",
    "    # XGBoost Modell\n",
    "    print(\"Trainiere XGBoost Regression...\")\n",
    "    model = XGBRegressor(\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=6,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        verbosity=1\n",
    "    )\n",
    "\n",
    "    # Modell trainieren\n",
    "    model.fit(X_train_transformed, y_train)\n",
    "\n",
    "    # Bewerten\n",
    "    y_pred = model.predict(X_test_transformed)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = mse ** 0.5\n",
    "    print(f\"Test RMSE auf 2.000 Testpunkten: {rmse:.2f}\")\n",
    "    print(\"Wahre vs. Vorhergesagte Preise (erste 10 Beispiele):\")\n",
    "    for true_val, pred_val in zip(y_test[:10], y_pred[:10]):\n",
    "        print(f\"Echter Preis: {true_val:.2f} - Vorhergesagt: {pred_val:.2f}\")\n",
    "\n",
    "    return model, preprocessor\n",
    "\n",
    "# Aufrufen der Funktion\n",
    "final_model, preprocessor = train_final_xgboost_regression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe254f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 2 - XGBoost mit Label Encoding\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from Preprocessing.split import split_data\n",
    "from Preprocessing.preprocessing_pipeline_initial import preprocessing_pipeline\n",
    "\n",
    "def train_final_xgboost_regression_label_encoding():\n",
    "    df = preprocessing_pipeline()\n",
    "    # Train-Test-Split\n",
    "    X_train, X_test, y_train, y_test, X, y, _, _ = split_data(df)\n",
    "\n",
    "    # -Stichprobe ziehen\n",
    "    print(\"Verwende nur Stichprobe (10.000 Train, 2.000 Test)\")\n",
    "    X_train = X_train.sample(n=10000, random_state=42)\n",
    "    y_train = y_train.loc[X_train.index]\n",
    "\n",
    "    X_test = X_test.sample(n=2000, random_state=42)\n",
    "    y_test = y_test.loc[X_test.index]\n",
    "\n",
    " \n",
    "    # Feature-Auflistung, evtl. für Fehlerfindung später\n",
    "    numeric_features = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "    categorical_features = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "    print(\"Erkannte numerische Features:\", numeric_features)\n",
    "    print(\"Erkannte kategorische Features:\", categorical_features)\n",
    "\n",
    "    # Diesmal Label-Encoding\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('ordinal', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1))\n",
    "    ])\n",
    "\n",
    "    preprocessor = ColumnTransformer(transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "    # Anwendung Preprocessing\n",
    "    print(\"Preprocessing wird angewendet...\")\n",
    "    X_train_transformed = preprocessor.fit_transform(X_train)\n",
    "    X_test_transformed = preprocessor.transform(X_test)\n",
    "\n",
    "    \n",
    "    # Trainieren des Modells\n",
    "    print(\"Trainiere XGBoost Regression...\")\n",
    "    model = XGBRegressor(\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=6,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        verbosity=1\n",
    "    )\n",
    "\n",
    "    model.fit(X_train_transformed, y_train)\n",
    "\n",
    "    # Bewerten \n",
    "    y_pred = model.predict(X_test_transformed)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = mse ** 0.5\n",
    "    print(f\"Test RMSE auf 2.000 Testpunkten: {rmse:.2f}\")\n",
    "    print(\"Wahre vs. Vorhergesagte Preise (erste 10 Beispiele):\")\n",
    "    for true_val, pred_val in zip(y_test[:10], y_pred[:10]):\n",
    "        print(f\"Echter Preis: {true_val:.2f} - Vorhergesagt: {pred_val:.2f}\")\n",
    "\n",
    "    return model, preprocessor\n",
    "\n",
    "# Anwenden der Funktion\n",
    "final_model, preprocessor = train_final_xgboost_regression_label_encoding()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707f8688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 3 - XGBoost mit Label-Encoding & Hyperparameteroptimierung RandomSearchCV\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from Preprocessing.split import split_data\n",
    "from Preprocessing.preprocessing_pipeline_initial import preprocessing_pipeline\n",
    "\n",
    "\n",
    "def train_final_xgboost_with_hyperparameter_tuning():\n",
    "    df = preprocessing_pipeline() \n",
    "    # Train-Test Split\n",
    "    X_train, X_test, y_train, y_test, X, y, _, _ = split_data(df) \n",
    "\n",
    "    # Stichprobe ziehen\n",
    "    print(\"Verwende nur Stichprobe (10.000 Train, 2.000 Test)\")\n",
    "    X_train = X_train.sample(n=10000, random_state=42)\n",
    "    y_train = y_train.loc[X_train.index]\n",
    "\n",
    "    X_test = X_test.sample(n=2000, random_state=42)\n",
    "    y_test = y_test.loc[X_test.index]\n",
    "\n",
    "    # Auflistung der Features\n",
    "\n",
    "    numeric_features = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "    categorical_features = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "    print(\"Erkannte numerische Features:\", numeric_features)\n",
    "    print(\"Erkannte kategorische Features:\", categorical_features)\n",
    "\n",
    "    # Skalierung, diesmal mit Label (Ordinal) Encoding\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('ordinal', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1))\n",
    "    ])\n",
    "\n",
    "    preprocessor = ColumnTransformer(transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "    # Anwendung Preprocessing\n",
    "    print(\"Preprocessing wird angewendet...\")\n",
    "    X_train_transformed = preprocessor.fit_transform(X_train)\n",
    "    X_test_transformed = preprocessor.transform(X_test)\n",
    "\n",
    "    # Hyperparameter-Optimierung mit RandomSearch\n",
    "    print(\"Starte RandomizedSearchCV für Hyperparameteroptimierung...\")\n",
    "\n",
    "    # Definition der Randoms\n",
    "    param_dist = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [3, 4, 5, 6, 7],\n",
    "        'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "        'subsample': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "        'colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "        'gamma': [0, 0.1, 0.2, 0.3],\n",
    "        'reg_alpha': [0, 0.01, 0.1],\n",
    "        'reg_lambda': [1, 1.5, 2]\n",
    "    }\n",
    "\n",
    "    xgb = XGBRegressor(random_state=42, n_jobs=-1, verbosity=1)\n",
    "\n",
    "    random_search = RandomizedSearchCV(\n",
    "        estimator=xgb,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=30,  # 30 Kombinationen testen\n",
    "        scoring='neg_root_mean_squared_error',\n",
    "        cv=3,\n",
    "        verbose=2,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    # Randomized Search ausführen\n",
    "    random_search.fit(X_train_transformed, y_train)\n",
    "\n",
    "    best_model = random_search.best_estimator_\n",
    "\n",
    "    print(\"Beste Hyperparameter:\")\n",
    "    print(random_search.best_params_)\n",
    "\n",
    "    # Bewertung mit Ergebnissen der Hyperparameteroptimierung\n",
    "    y_pred = best_model.predict(X_test_transformed)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = mse ** 0.5\n",
    "    print(f\"Test RMSE auf 2.000 Testpunkten: {rmse:.2f}\")\n",
    "    print(\"\\n✅ Wahre vs. Vorhergesagte Preise (erste 10 Beispiele):\")\n",
    "    for true_val, pred_val in zip(y_test[:10], y_pred[:10]):\n",
    "        print(f\"Echter Preis: {true_val:.2f} - Vorhergesagt: {pred_val:.2f}\")\n",
    "\n",
    "    return best_model, preprocessor\n",
    "\n",
    "\n",
    "# Aufrufen der Funktion\n",
    "final_model, preprocessor = train_final_xgboost_with_hyperparameter_tuning()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a7a026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 4 - XGBoost mit One-Hot-Encoding & Hyperparameteroptimierung RandomSearchCV\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "from Preprocessing.split import split_data\n",
    "from Preprocessing.preprocessing_pipeline_initial import preprocessing_pipeline\n",
    "\n",
    "\n",
    "def train_final_xgboost_with_hyperparameter_tuning_onehot():\n",
    "    df = preprocessing_pipeline()\n",
    "    # Train-Test Split\n",
    "    X_train, X_test, y_train, y_test, X, y, _, _ = split_data(df)\n",
    "\n",
    "    # Stichprobe ziehen\n",
    "    print(\"Verwende nur Stichprobe (10.000 Train, 2.000 Test) für schnelleren Lauf!\")\n",
    "    X_train = X_train.sample(n=10000, random_state=42)\n",
    "    y_train = y_train.loc[X_train.index]\n",
    "\n",
    "    X_test = X_test.sample(n=2000, random_state=42)\n",
    "    y_test = y_test.loc[X_test.index]\n",
    "\n",
    "    # Auflistung der Features\n",
    "    numeric_features = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "    categorical_features = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "    print(\"Erkannte numerische Features:\", numeric_features)\n",
    "    print(\"Erkannte kategorische Features:\", categorical_features)\n",
    "\n",
    "    # Skalierung, diesmal mit One-Hot Encoding\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "    ])\n",
    "\n",
    "    preprocessor = ColumnTransformer(transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "    # Anwendung Preprocessing\n",
    "    print(\"Preprocessing wird angewendet...\")\n",
    "    X_train_transformed = preprocessor.fit_transform(X_train)\n",
    "    X_test_transformed = preprocessor.transform(X_test)\n",
    "\n",
    "    # Hyperparameteroptimierung mit RandomizedSearch\n",
    "\n",
    "    print(\"Starte RandomizedSearchCV für Hyperparameteroptimierung...\")\n",
    "\n",
    "    # Definition der Random Parameter\n",
    "    param_dist = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [3, 4, 5, 6, 7],\n",
    "        'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "        'subsample': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "        'colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "        'gamma': [0, 0.1, 0.2, 0.3],\n",
    "        'reg_alpha': [0, 0.01, 0.1],\n",
    "        'reg_lambda': [1, 1.5, 2]\n",
    "    }\n",
    "\n",
    "    xgb = XGBRegressor(random_state=42, n_jobs=-1, verbosity=1)\n",
    "\n",
    "    random_search = RandomizedSearchCV(\n",
    "        estimator=xgb,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=30,  # 30 Kombinationen testen\n",
    "        scoring='neg_root_mean_squared_error',\n",
    "        cv=3,\n",
    "        verbose=2,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    # Randomized Search ausführen\n",
    "    random_search.fit(X_train_transformed, y_train)\n",
    "\n",
    "    best_model = random_search.best_estimator_\n",
    "\n",
    "    print(\"Beste Hyperparameter:\")\n",
    "    print(random_search.best_params_)\n",
    "\n",
    "    # Bewertung mit Ergebnissen aus RandomSearch\n",
    "    y_pred = best_model.predict(X_test_transformed)\n",
    "\n",
    "    \n",
    "\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = mse ** 0.5\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    print(f\"Test RMSE auf 2.000 Testpunkten: {rmse:.2f}\")\n",
    "    print(f\"Test MSE: {mse:.2f}\")\n",
    "    print(f\"Test MAE: {mae:.2f}\")\n",
    "    print(f\"Test R²: {r2:.4f}\")\n",
    "    print(\"Wahre vs. Vorhergesagte Preise (erste 10 Beispiele):\")\n",
    "    for true_val, pred_val in zip(y_test[:10], y_pred[:10]):\n",
    "        print(f\"Echter Preis: {true_val:.2f} - Vorhergesagt: {pred_val:.2f}\")\n",
    "\n",
    "    return best_model, preprocessor\n",
    "\n",
    "# Aufrufen der Funktion\n",
    "final_model, preprocessor = train_final_xgboost_with_hyperparameter_tuning_onehot()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4812bcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 5 - XGBoost mit Target-Hot-Encoding & Hyperparameteroptimierung RandomSearchCV\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from xgboost import XGBRegressor\n",
    "import joblib\n",
    "from category_encoders import TargetEncoder\n",
    "\n",
    "from Preprocessing.split import split_data\n",
    "from Preprocessing.preprocessing_pipeline_initial import preprocessing_pipeline\n",
    "\n",
    "\n",
    "def train_final_xgboost_with_target_encoding():\n",
    "    df = preprocessing_pipeline()\n",
    "    # Train-Test Split\n",
    "    X_train, X_test, y_train, y_test, X, y, _, _ = split_data(df) \n",
    "\n",
    "    # Stichprobe ziehen\n",
    "    print(\"Verwende nur Stichprobe\")\n",
    "    X_train = X_train.sample(n=10000, random_state=42)\n",
    "    y_train = y_train.loc[X_train.index]\n",
    "\n",
    "    X_test = X_test.sample(n=2000, random_state=42)\n",
    "    y_test = y_test.loc[X_test.index]\n",
    "\n",
    "    # Feature Auflistung\n",
    "    numeric_features = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "    categorical_features = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "    print(\"\\n✅ Erkannte numerische Features:\", numeric_features)\n",
    "    print(\"\\n✅ Erkannte kategorische Features:\", categorical_features)\n",
    "\n",
    "    # Preprocessing für numerische Features\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    # Preprocessing für kategorische Features (Target Encoding)\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('target', TargetEncoder())\n",
    "    ])\n",
    "\n",
    "    preprocessor = ColumnTransformer(transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "    # Preprocessing Anwendung\n",
    "    print(\"Preprocessing wird auf Trainingsdaten angewendet...\")\n",
    "    X_train_transformed = preprocessor.fit_transform(X_train, y_train)  # <- Wichtig: Target (y_train) für TargetEncoder!\n",
    "    X_test_transformed = preprocessor.transform(X_test)\n",
    "\n",
    "    # Hyperparameteroptimierung mit RandomizedSearchCV\n",
    "\n",
    "    print(\"Starte RandomizedSearchCV für Hyperparameteroptimierung...\")\n",
    "    param_dist = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [3, 4, 5, 6, 7],\n",
    "        'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "        'subsample': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "        'colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "        'gamma': [0, 0.1, 0.2, 0.3],\n",
    "        'reg_alpha': [0, 0.01, 0.1],\n",
    "        'reg_lambda': [1, 1.5, 2]\n",
    "    }\n",
    "\n",
    "    xgb = XGBRegressor(random_state=42, n_jobs=-1, verbosity=1)\n",
    "\n",
    "    random_search = RandomizedSearchCV(\n",
    "        estimator=xgb,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=30,  # 30 Kombinationen testen\n",
    "        scoring='neg_root_mean_squared_error',\n",
    "        cv=3,\n",
    "        verbose=2,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    # Randomized Search ausführen\n",
    "    random_search.fit(X_train_transformed, y_train)\n",
    "    best_model = random_search.best_estimator_\n",
    "\n",
    "    print(\"Beste Hyperparameter:\")\n",
    "    print(random_search.best_params_)\n",
    "\n",
    "    # Bewertung mit Ergebnissen aus RandomSearch\n",
    "    y_pred = best_model.predict(X_test_transformed)\n",
    "\n",
    "    from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = mse ** 0.5\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    print(f\"Test RMSE auf 2.000 Testpunkten: {rmse:.2f}\")\n",
    "    print(f\"Test MSE: {mse:.2f}\")\n",
    "    print(f\"Test MAE: {mae:.2f}\")\n",
    "    print(f\"Test R²: {r2:.4f}\")\n",
    "    print(\" Wahre vs. Vorhergesagte Preise (erste 10 Beispiele):\")\n",
    "    for true_val, pred_val in zip(y_test[:10], y_pred[:10]):\n",
    "        print(f\"Echter Preis: {true_val:.2f} - Vorhergesagt: {pred_val:.2f}\")\n",
    "\n",
    "    return best_model, preprocessor\n",
    "\n",
    "# Aufrufen der Funktion\n",
    "final_model, preprocessor = train_final_xgboost_with_target_encoding()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Data Mining Project Kernel",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
