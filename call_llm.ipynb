{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b48ac7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import time\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np \n",
    "from Preprocessing.preprocessing_pipeline_initial import preprocessing_pipeline\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "load_dotenv()  # lädt .env automatisch\n",
    "os.environ[\"GOOGLE_API_KEY\"] = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59973e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardmäßig werden alle Zeilen angezeigt, egal, wie viele es sind\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b673c2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef use_llm_on_model(model_list):\\n    # Diese Funktion ruft die call_llm Funktion auf, um der Spalte model das Fahrzeugsegment zuzuordnen\\n\\n    result = {}\\n    counter = 0\\n    limit_per_minute = 15 # brauchen diesen Timer um nicht zu viele Anfragen zu stellen (free sind 15 Anfragen/Min)\\n\\n    for idx, model in enumerate(model_list, start=1):\\n        result[model] = call_llm(model)\\n        counter += 1\\n\\n        # Warten, wenn das Limit erreicht ist\\n        if counter % limit_per_minute == 0:\\n            print(f\"{counter} Anfragen gestellt – warte 60 Sekunden, um Rate Limit einzuhalten...\")\\n            time.sleep(60)\\n\\n    print(\"Fertig!\")\\n    return result\\n\\ndef use_llm_on_model(model_list):\\n    # Diese Funktion ruft die call_llm Funktion auf, um der Spalte model das Fahrzeugsegment zuzuordnen\\n\\n    result = {}\\n\\n    for model in (model_list):\\n        result[model] = call_llm(model)\\n\\n    return result\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import concurrent.futures\n",
    "\n",
    "def call_llm(text):\n",
    "    # Diese Funktion nutzt die API von Google um sich gegen das LLM zu schalten. Prompt etc. ist unten zu sehen\n",
    "\n",
    "    llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash-001\", #\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2\n",
    "    )\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                '''Du bist ein Experte für Automobilklassifikation. Deine Aufgabe ist es, Automodelle basierend auf ihrem Namen einem von fünf vordefinierten Fahrzeugsegmenten zuzuordnen. \n",
    "                    Bitte gib nur den Namen des passenden Segments zurück.\n",
    "                    Die Segmente sind:\n",
    "                    1. Kleinwagen – kleine Stadtautos, z.B. VW Up!, Renault Clio, Fiat Panda\n",
    "                    2. Mittelklasse – normale Alltagsautos, z.B. VW Golf, Audi A4, BMW 3er\n",
    "                    3. Geländewagen – große Fahrzeuge mit viel Platz, z.B. BMW X3, VW Tiguan, Ford Kuga, VW Multivan\n",
    "                    4. Sportwagen – sportliche Fahrzeuge mit viel PS, z.B. Porsche 911, Audi R8, BMW M4\n",
    "                    5. Luxusklasse – hochwertige Fahrzeuge mit Premiumausstattung, z.B. BMW 7er, Mercedes S-Klasse, Tesla Model S\n",
    "                    Beispiele:\n",
    "\n",
    "                    Input: smart forTwo\n",
    "                    Output: Kleinwagen\n",
    "                    Input: Volkswagen Golf\n",
    "                    Output: Mittelklasse\n",
    "                    Input: BMW X5\n",
    "                    Output: Geländewagen\n",
    "                    Input: Porsche 911\n",
    "                    Output: Sportwagen\n",
    "                    Input: Bentley Mulsanne\n",
    "                    Output: Luxusklasse\n",
    "                Gib mir als Output nur das Fahrzeugsegment aus.                          \n",
    "                ''',\n",
    "            ),\n",
    "            (\"human\", \"{input}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    chain = prompt | llm\n",
    "    res = chain.invoke(\n",
    "        {\n",
    "            \"input\": text\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return res.content\n",
    "\n",
    "\n",
    "\n",
    "def use_llm_on_model_parallel(model_list, max_workers=5):\n",
    "    result = {}\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_model = {executor.submit(call_llm, model): model for model in model_list}\n",
    "        \n",
    "        for future in concurrent.futures.as_completed(future_to_model):\n",
    "            model = future_to_model[future]\n",
    "            try:\n",
    "                result[model] = future.result()\n",
    "            except Exception as e:\n",
    "                result[model] = f\"Error: {e}\"\n",
    "\n",
    "    return result\n",
    "'''\n",
    "model_list = df['model'].unique().tolist()\n",
    "result = use_llm_on_model_parallel(model_list)\n",
    "df[\"segment\"] = df[\"model\"].map(result).fillna(\"unknown\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6199d5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = preprocessing_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e861e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up LLM chain...\n",
      "LLM chain ready.\n",
      "Starting parallel classification for 343 unique models...\n",
      "Parallel classification finished.\n",
      "\n",
      "Sample Results:\n",
      "{'audi Audi TT': 'Sportwagen', 'audi Audi A8': 'Oberklasse/Luxusklasse', 'audi Audi S6': 'Obere Mittelklasse', 'audi Audi A4': 'Mittelklasse', 'audi Audi A3': 'Kompaktklasse', 'audi Audi A6': 'Obere Mittelklasse', 'alfa-romeo Alfa Romeo Giulietta': 'Kompaktklasse', 'alfa-romeo Alfa Romeo Stelvio': 'SUV', 'audi Audi S3': 'Kompaktklasse', 'audi Audi A6 allroad': 'Obere Mittelklasse'}\n"
     ]
    }
   ],
   "source": [
    "import concurrent.futures\n",
    "import functools # Wird benötigt, um Argumente an die Funktion im Thread zu binden\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "\n",
    "# Deine 8 finalen Segmente\n",
    "segments = [\n",
    "    \"Kleinwagen\",\n",
    "    \"Kompaktklasse\",\n",
    "    \"Mittelklasse\",\n",
    "    \"Obere Mittelklasse\",\n",
    "    \"Oberklasse/Luxusklasse\",\n",
    "    \"SUV\",\n",
    "    \"Van\",\n",
    "    \"Sportwagen\"\n",
    "]\n",
    "VALID_SEGMENTS_UPDATED = segments # Für die Validierung\n",
    "\n",
    "# Neuer System-Prompt (gekürzt zur Übersicht, nimm den vollständigen von oben)\n",
    "system_prompt = f'''Du bist ein Experte für Automobilklassifikation in Deutschland. Deine Aufgabe ist es, Automodelle (inkl. Marke) einem der folgenden {len(segments)} vordefinierten Fahrzeugsegmenten zuzuordnen.\n",
    "Bitte gib nur den Namen des passenden Segments zurück. Stelle sicher, dass deine Antwort EXAKT einem der unten genannten Segmente entspricht.\n",
    "\n",
    "Die Segmente und Beispiele sind:\n",
    "\n",
    "1.  Kleinwagen: Kleine Stadtflitzer und Superminis.\n",
    "     z.B.: VW Polo, Opel Corsa, Ford Fiesta, Renault Clio, Peugeot 208, Toyota Yaris, Skoda Fabia, Fiat 500, Mini Cooper, Hyundai i20, Seat Ibiza\n",
    "\n",
    "2.  Kompaktklasse: Die \"Golf-Klasse\", untere Mittelklasse.\n",
    "     z.B.: VW Golf, Audi A3, BMW 1er, Mercedes A-Klasse, Opel Astra, Ford Focus, Seat Leon, Skoda Octavia, Hyundai i30, Kia Ceed\n",
    "\n",
    "3.  Mittelklasse: Standard-Limousinen und Kombis für Familie/Beruf.\n",
    "     z.B.: VW Passat, Audi A4, BMW 3er, Mercedes C-Klasse, Skoda Superb, Ford Mondeo, Opel Insignia, Volvo S60/V60, Tesla Model 3\n",
    "\n",
    "4.  Obere Mittelklasse: Größere Geschäfts- und Reisefahrzeuge.\n",
    "     z.B.: Audi A6, BMW 5er, Mercedes E-Klasse, Volvo S90/V90, Jaguar XF, Lexus ES\n",
    "\n",
    "5.  Oberklasse/Luxusklasse: Repräsentative Luxusfahrzeuge.\n",
    "     z.B.: Audi A8, BMW 7er, Mercedes S-Klasse, Porsche Panamera, Lexus LS, Bentley Continental GT, Tesla Model S\n",
    "\n",
    "6.  SUV: Sport Utility Vehicles, verschiedene Größen, oft mit erhöhter Sitzposition.\n",
    "    z.B.: VW Tiguan, VW Touareg, Audi Q3/Q5/Q7, BMW X1/X3/X5, Mercedes GLA/GLC/GLE, Ford Kuga, Skoda Kodiaq/Karoq, Porsche Macan/Cayenne, Volvo XC60, Dacia Duster\n",
    "\n",
    "7.  Van: Familienvans, Kleinbusse mit viel Platz.\n",
    "     z.B.: VW Touran, VW Sharan, VW Multivan/Caravelle, Mercedes V-Klasse/Vito Tourer, Ford Galaxy/S-Max, Opel Zafira, Seat Alhambra, Renault Espace\n",
    "\n",
    "8.  Sportwagen: Leistungsstarke Coupés, Roadster oder Sportlimousinen.\n",
    "     z.B.: Porsche 911, Porsche 718 Cayman/Boxster, Audi R8, Audi TT, BMW M2/M3/M4, Mercedes-AMG GT/SL, Ford Mustang, Jaguar F-Type, Nissan GT-R\n",
    "\n",
    "Gib mir als Output NUR das EINE passende Fahrzeugsegment aus der Liste {segments} zurück. KEINEN zusätzlichen Text.\n",
    "'''\n",
    "\n",
    "def setup_llm_chain_updated():\n",
    "    \"\"\"Initialisiert das LLM und die Prompt Chain mit den Segmenten.\"\"\"\n",
    "    llm = ChatGoogleGenerativeAI(\n",
    "        model=\"gemini-2.0-flash-001\", # anderes Modell noch testen\n",
    "        temperature=0, # Kreativität\n",
    "        max_tokens=50, #länge der Antwort (kurz da nur Segment erwartet)\n",
    "        timeout=None,\n",
    "        max_retries=2\n",
    "    )\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system_prompt),\n",
    "            (\"human\", \"{input}\"), # {input} sollte \"Marke Modell\" sein\n",
    "        ]\n",
    "    )\n",
    "    chain = prompt | llm\n",
    "    return chain\n",
    "\n",
    "# --- 2. Die Funktion, die EINEN EINZELNEN LLM-Aufruf macht (angepasst) ---\n",
    "# Diese Funktion wird von den parallelen Threads aufgerufen.\n",
    "# Sie bekommt jetzt die vorkonfigurierte Chain übergeben.\n",
    "\n",
    "def call_llm_single_validated(model_input_string, chain_instance):\n",
    "    \"\"\"\n",
    "    Ruft die LLM-Chain für einen einzelnen Modell-String auf und validiert das Ergebnis.\n",
    "\n",
    "    Args:\n",
    "        model_input_string: Der Input für das LLM (Format \"Marke Modell\").\n",
    "        chain_instance: Die vorkonfigurierte LangChain-Chain.\n",
    "\n",
    "    Returns:\n",
    "        Das validierte Segment (String) oder eine Fehlermeldung (String).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Rufe die Chain für den einzelnen Input auf\n",
    "        response = chain_instance.invoke({\"input\": model_input_string})\n",
    "\n",
    "        if isinstance(response, AIMessage):\n",
    "            segment = response.content.strip()\n",
    "            # Validierung gegen die erlaubten Segmente\n",
    "            if segment in VALID_SEGMENTS_UPDATED:\n",
    "                return segment\n",
    "            else:\n",
    "                # LLM hat ein ungültiges Segment zurückgegeben\n",
    "                print(f\"Warning: Invalid segment '{segment}' received for '{model_input_string}'\")\n",
    "                return f\"Error: Invalid Segment - '{segment}'\" # Oder 'Sonstige'/None\n",
    "        else:\n",
    "            # Unerwarteter Rückgabetyp\n",
    "            print(f\"Warning: Unexpected output type {type(response)} for '{model_input_string}'\")\n",
    "            return f\"Error: Unexpected Output Type\"\n",
    "\n",
    "    except Exception as e:\n",
    "        # Fehler während des LLM-Aufrufs selbst\n",
    "        print(f\"Error calling LLM for '{model_input_string}': {e}\")\n",
    "        return f\"Error: API Call Failed - {e}\"\n",
    "\n",
    "# --- 3. Deine parallele Ausführungsfunktion (leicht angepasst) ---\n",
    "# Sie muss die vorkonfigurierte Chain erhalten und an call_llm_single_validated übergeben.\n",
    "\n",
    "def use_llm_on_model_parallel_updated(model_strings_list, chain_to_use, max_workers=5):\n",
    "    \"\"\"\n",
    "    Nutzt ThreadPoolExecutor, um call_llm_single_validated parallel für eine Liste\n",
    "    von Modell-Strings aufzurufen.\n",
    "\n",
    "    Args:\n",
    "        model_strings_list: Liste von Strings (Format \"Marke Modell\").\n",
    "        chain_to_use: Die vorkonfigurierte LangChain-Chain, die verwendet werden soll.\n",
    "        max_workers: Maximale Anzahl paralleler Threads.\n",
    "\n",
    "    Returns:\n",
    "        Ein Dictionary, das Input-Strings auf Ergebnisse abbildet.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "\n",
    "    # functools.partial wird verwendet, um die 'chain_to_use' fest an\n",
    "    # call_llm_single_validated zu binden. Der Executor übergibt dann nur noch\n",
    "    # das jeweilige 'model_string' aus der Liste.\n",
    "    func_to_call = functools.partial(call_llm_single_validated, chain_instance=chain_to_use)\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Sende Jobs an den Executor\n",
    "        # Map Input-String zum Future-Objekt\n",
    "        future_to_model = {executor.submit(func_to_call, model_str): model_str for model_str in model_strings_list}\n",
    "\n",
    "        # Sammle Ergebnisse, sobald sie fertig sind\n",
    "        for future in concurrent.futures.as_completed(future_to_model):\n",
    "            model_str = future_to_model[future]\n",
    "            try:\n",
    "                # Hole das Ergebnis vom Future (dies kann auch eine Exception auslösen,\n",
    "                # falls call_llm_single_validated eine Exception wirft, was wir aber\n",
    "                # durch try/except innerhalb der Funktion meist vermeiden)\n",
    "                result_segment = future.result()\n",
    "                results[model_str] = result_segment\n",
    "            except Exception as exc:\n",
    "                # Falls doch eine unerwartete Exception aus future.result() kommt\n",
    "                print(f'Model {model_str} generated an exception: {exc}')\n",
    "                results[model_str] = f\"Error: Exception - {exc}\"\n",
    "\n",
    "    return results\n",
    "\n",
    "# --- 4. Beispiel-Anwendung ---\n",
    "\n",
    "# WICHTIG: Stelle sicher, dass deine Input-Liste Strings im Format \"Marke Modell\" enthält!\n",
    "# Beispiel:\n",
    "# model_list = [\"Volkswagen Golf\", \"Porsche 911\", \"Mercedes-Benz G 63 AMG\", \"Fiat 500\", \"BMW X5\", \"Ford Mustang\"]\n",
    "# Ersetze dies durch deine tatsächliche Liste von Modell-Strings\n",
    "unique_model_brand_strings = list(df[['brand', 'model']].apply(lambda x: f\"{x['brand']} {x['model']}\", axis=1).unique())\n",
    "\n",
    "# a) Erstelle die LLM-Chain EINMALIG\n",
    "print(\"Setting up LLM chain...\")\n",
    "my_llm_chain = setup_llm_chain_updated()\n",
    "print(\"LLM chain ready.\")\n",
    "\n",
    "# b) Rufe die parallele Funktion auf und übergib die Chain\n",
    "print(f\"Starting parallel classification for {len(unique_model_brand_strings)} unique models...\")\n",
    "classification_results = use_llm_on_model_parallel_updated(\n",
    "    model_strings_list=unique_model_brand_strings,\n",
    "    chain_to_use=my_llm_chain,\n",
    "    max_workers=10 # Passe die Worker-Anzahl nach Bedarf an\n",
    ")\n",
    "print(\"Parallel classification finished.\")\n",
    "\n",
    "# c) Ergebnisse anzeigen (Auszug)\n",
    "print(\"\\nSample Results:\")\n",
    "print(dict(list(classification_results.items())[:10]))\n",
    "\n",
    "# d) Ergebnisse zurück in den DataFrame mappen (Beispiel)\n",
    "print(\"\\nMapping results back to DataFrame...\")\n",
    "df['segment'] = df.apply(lambda x: classification_results.get(f\"{x['brand']} {x['model']}\", \"Error: Mapping Failed\"), axis=1)\n",
    "print(df[['brand', 'model', 'segment']].head())\n",
    "print(f\"\\nValue counts for segments:\\n{df['segment'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a790452",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('df_mit_segmenten.csv', index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95d57b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f35ad61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_mining_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
