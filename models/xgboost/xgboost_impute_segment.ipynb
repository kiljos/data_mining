{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c700272f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "import xgboost as xgb\n",
    "import gc\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"../..\"))\n",
    "\n",
    "\n",
    "from Preprocessing.preprocessing_pipeline_impute import preprocessing_pipeline\n",
    "from Preprocessing.preprocessing_pipeline_segment import preprocessing_pipeline_segment\n",
    "from Preprocessing.imputation import get_imputation_maps, apply_imputation,ContextImputer\n",
    "from Preprocessing.preprocessing_pipeline_segment import preprocessing_pipeline_segment\n",
    "from Preprocessing.split import split_data\n",
    "from eval_call import evaluate_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb67ca6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training XGBoost model...\n",
      "XGBoost Performance Metrics:\n",
      "MAE: 3560.60\n",
      "MSE: 85714576.00\n",
      "RMSE: 9258.22\n",
      "RÂ²: 0.92\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "#Decision trees --> = XGBoost model (xgb ist ein besserer Decision Tree)\n",
    "\n",
    "def main():\n",
    "    df = preprocessing_pipeline('../../data.csv')\n",
    "    df = preprocessing_pipeline_segment(df)\n",
    "    X_train, X_test, y_train, y_test, X, y, categorical_features, numeric_features = split_data(df)\n",
    "    \n",
    "    \n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numeric_features),\n",
    "            ('cat', categorical_transformer, categorical_features)\n",
    "        ])\n",
    "\n",
    "    # Create feature names list for later use in importance analysis\n",
    "    feature_names = numeric_features.copy()\n",
    "    \n",
    "    # Get categorical encoded feature names\n",
    "    preprocessor.fit(X_train)\n",
    "    try:\n",
    "        categorical_encoded_features = preprocessor.named_transformers_['cat'].named_steps['onehot'].get_feature_names_out(categorical_features)\n",
    "        feature_names.extend(categorical_encoded_features)\n",
    "    except:\n",
    "        print(\"Warning: Could not get encoded feature names\")\n",
    "\n",
    "    # Create XGBoost pipeline\n",
    "    xgb_pipeline = Pipeline(steps=[\n",
    "        ('imp_fc', ContextImputer('fuel_consumption_l_100km')),\n",
    "        ('imp_ps', ContextImputer('power_ps')),\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', xgb.XGBRegressor(\n",
    "            n_estimators=200,\n",
    "            learning_rate=0.1,\n",
    "            max_depth=7,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            random_state=42,\n",
    "            n_jobs=-1  # Use all available cores\n",
    "        ))\n",
    "    ])\n",
    "    \n",
    "    # Train model\n",
    "    print(\"Training XGBoost model...\")\n",
    "    xgb_pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_xgb = xgb_pipeline.predict(X_test)\n",
    "    \n",
    "    \n",
    "    evaluate_model(y_test, y_pred_xgb, \"XGBoost\")\n",
    "    \n",
    "   \n",
    "    # Clean up memory\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_mining_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
